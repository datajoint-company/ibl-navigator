<!-- <div class="user-guide-container">
<h4>User Guide</h4>
  <iframe width="100%" height="800px"
    src="https://docs.google.com/document/d/e/2PACX-1vRG2kp5vLlRmnbo1kYkoJmmfBHWuylsDXHT0N_teP27OGyctip5_SXaxlqCaIiqGx-FysgwMP68OpHK/pub?embedded=true"></iframe>
</div> -->
<!-- <img class="home_banner" src="assets/images/home_ibl_brainbow.jpg"> -->
<div class="home-container">
  <div class="home-top-container">
    <div class="top-text-container">
      <h2 class="top-title">International Brain Laboratory Behavioral Data Portal</h2>
      <div class="top-text-details">
        <p class="detail-text">The <a href="https://www.internationalbrainlab.com" target="_blank">International Brain Laboratory</a> is a team of 
          systems and computational neuroscientists, working collaboratively to understand the computations that support decision-making in the brain. 
          Through this portal you can access data gathered while mice made decisions that combine incoming visual evidence with internal beliefs 
          about the dynamic structure of the environment. 
        </p>
        <p class="detail-text">You can view on this portal two types of data:</p>
        <ol class="detail-text">
          <li>
            <b>Electrophysiology data</b> recorded in the mouse brain during behavior. Neurons are recorded at various locations within the brain 
            using Neuropixels probes, once mice are experts in the behavioural paradigm described below. Through this portal, users can view the 
            properties of the recorded neurons (e.g. waveforms), as well as their responses to different stimuli and events ongoing during behavior.
            <br />
            <br />
            The content of this portal reflects the electrophysiology data associated with 4 recordings, each acquired at a different institution, 
            which serve as a teaser in preparation for the full data release expected in 2022.
          </li>
          <li>
            <b>Behavioral data</b> from a standardized training pipeline, implemented across 9 labs in 7 institutions. Mice learn to make decisions 
            that combine incoming visual evidence with internal beliefs about the dynamic structure of the environment. Through this portal, users 
            can view behavioral data from mice throughout their training, and see the transition from novice to expert behavior unfold.
            <br />
            <br />
            The content of this portal reflects the behavioral data associated with 198 mice up until 2020-03-23, as used in 
            <a target="_blank" href="https://doi.org/10.1101/2020.01.17.909838">The International Brain Laboratory et al. 2020</a>, plus the 
            behavioral data associated with the newly released 4 electrophysiology recordings.
          </li>
        </ol>
      </div>
    </div>
  </div>

  <div class="home-shaded-container">
    <div class="detail-container">
      <h3>How can you access the data?</h3>
      <ol class="detail-text">
        <li>To access the <b>data associated with the electrophysiology recordings</b>, follow <a href="https://int-brain-lab.github.io/iblenv/public_docs/public_one.html" target="_blank">
          these guidelines</a>.
        </li>
        <li>To access the <b>behavior data</b> used in this online portal, you have three options:
          <ol style="list-style-type: lower-alpha">
            <li><b>Access the DataJoint database via <a target="_blank" href="https://jupyterhub.internationalbrainlab.org">JupyterHub</a>:</b> this site hosts 
              several notebooks that allow you to directly interact with the database with DataJoint. You do not need to download or install anything locally.
              <ol style="list-style-type: lower-roman">
                <li>Log in with your GitHub account.</li>
                <li>Read the README</li>
                <li>Click on <span class="code-style">public_notebooks/Explore IBL pipeline</span> to run several notebooks (one of which replicates 
                  figure 2 of <a target="_blank" href="https://doi.org/10.1101/2020.01.17.909838">The International Brain Laboratory et al. 2020</a>).</li>
              </ol>
            </li>
            <br>
            <li><b>Access the database locally with <a target="_blank" href="https://datajoint.org">DataJoint</a>.</b>
              <ol style="list-style-type: lower-roman">
                <li>See detailed instructions on <a href="https://int-brain-lab.github.io/iblenv/public_docs/public_datajoint.html" target="_blank">accessing the database locally</a></li>
              </ol>
            </li>
            <br>
            <li>You may also directly download the data through your web browser without running any code. Simply load an internet browser and use the provided link from the <a href="http://ibl.flatironinstitute.org/public/behavior_paper_data.zip" target="_blank"><b>Behavior paper data URL</b></a>
            </li>
            <br>
          </ol>
        </li>
      </ol>
      <p class="detail-text">Issues with accessing the data? Email <a href="mailto:info@internationalbrainlab.org" target="_blank">info@internationalbrainlab.org</a>.</p>
      <p class="detail-text">General questions about the paper <a target="_blank" href="https://doi.org/10.1101/2020.01.17.909838">The International Brain Laboratory et al. 2020</a>? 
        Email <a href="mailto:info+behavior@internationalbrainlab.org" target="_blank">info+behavior@internationalbrainlab.org</a>.
      </p>
    </div>

    <!-- temporary for NMA events -->
    <!-- <div class="detail-container"> 
      <h3>How can you view and access the data?</h3>
      <ol class="detail-text">
        <li>On this online portal, you can view and browse the data by clicking on ‘mice’ and ‘sessions’ above.</li>
        <li>To access the data via <a target="_blank" href="https://datajoint.io">DataJoint</a>, we prepared a <a href="https://github.com/int-brain-lab/nma-ibl" target="_blank">mini environment</a> for you to
          access the database. Please visit and register your email at <a href="https://datajoint.io/events/nma-ibl-public" target="_blank">DataJoint.io NMA Public Access</a> to receive the access credentials to
          the database. Then you will be able to fill in the credentials in the following notebooks hosted on colab and explore the database: <br>
          <a href="https://colab.research.google.com/github/int-brain-lab/nma-ibl/blob/master/01-Explore%20IBL%20behavior%20data%20pipeline.ipynb" target="_blank">Explore IBL behavior data pipeline</a> <br>
          <a href="https://colab.research.google.com/github/int-brain-lab/nma-ibl/blob/master/02-Plot%20Psychometric%20curve.ipynb" target="_blank">Plot psychometric curve</a> <br>
          <a href="https://colab.research.google.com/github/int-brain-lab/nma-ibl/blob/master/03-Replication%20of%20paper%20figures.ipynb" target="_blank">Replication of paper figures</a> <br>
          
        </li>
        <li>Lastly, the data used for plotting can be accessed from the GitHub repository:  <a target="_blank" href="https://github.com/int-brain-lab/paper-behavior">https://github.com/int-brain-lab/paper-behavior</a></li>
      </ol>
      <p class="detail-text">Issues with data access? Create an issue at https://github.com/int-brain-lab/paper-behavior. General questions about the paper or data? Email data+behavior@internationalbrainlab.org. </p>
    </div> -->
  </div>

  <div class="home-light-container">
    <div class="detail-container">
      <h3>Behavioral Paradigm</h3>
      <p class="detail-text">The IBL behavioral data is generated during a visual decision-making task for mice. Mice are trained to judge the spatial location 
        of a visual stimulus (vertical grating) and report their decision by turning a wheel to move the stimulus into the center of the screen. The contrast 
        of the visual stimulus can be varied, so that decisions range from easy to ambiguous. This allows quantification of the animals' threshold, bias, and 
        lapse rates (below). Behavior during decision-making is characterized both using traditional metrics, such as choice and reaction time, along with video 
        recordings from high-speed cameras.</p>
      <div class="figures col-10 offset-1">
        <div class="small-figures">
          <div class="each-figure">
            <img src="assets/images/fig1.png" class="text-image" id="fig1">
            <p class="figure-caption"><i>Left-Right lick based behavioral paradigm</i></p>
          </div>
          <div class="each-figure">
            <img src="assets/images/fig2.png" class="text-image" id="fig2">
            <p class="figure-caption"><i>Left-Right bias conditions</i></p>
          </div>
        </div>
        <div class="each-figure">
          <img src="assets/images/fig2b.png" class="text-image-wide" id="fig3b">
          <p class="figure-caption-long"><i>Left: Average psychometric curve for each laboratory. Circles show the mean and error bars ± 1 the S.D. 
            Middle: Psychometric curves shift between biased blocks averaged over all animals. For each animal and signed contrast, we computed 
            their ‘bias shift’ (Δ) by reading out the difference in choice fraction between the 80:20 and 20:80 blocks (dashed lines). 
            Right: Average shift in rightward choices as a function of signed contrast for each laboratory (colors as in c; error bars show 
            mean +- 68% CI)</i></p>
        </div>
        
        <!-- <div class="each-figure">
          <img src="assets/images/fig3.png" class="text-image" id="fig3">
          <p class="figure-caption"><i>Psychometric curves</i></p>
        </div> -->
        
      </div>
      <div class="figures col-10 col-lg-8 col-xl-6 offset-1">
        <div class="each-figure">
          <img src="assets/images/PMFSchematic.png" class="" id="fig4">
          <p class="figure-caption"><i>Animals' bias, threshold, and lapse rate</i></p>
        </div>
      </div>
      <!-- <ol>
        <li><a href="https://doi.org/10.1016/j.neuron.2017.12.013" target="_blank">Abbott, L. F. <i>et al.</i> An International Laboratory for Systems and Computational Neuroscience. <i>Neuron </i> <b>96</b>, 1213–1218
        (2017).</a></li>
        <li><a href="https://doi.org/10.1016/j.celrep.2017.08.047" target="_blank">Burgess, C. P. <i>et al</i>. High-Yield Methods for Accurate Two-Alternative Visual Psychophysics in Head-Fixed Mice. <i>Cell Rep. </i>
        <b>20</b>, 2513–2524 (2017).</a></li>
      </ol> -->
    </div>
  </div>

  <div class="home-shaded-container">
    <div class="detail-container">
      <h3>Training Criteria</h3>
      <p class="detail-text">
        We present here a summary of training stages and a list of criteria. For exact definitions and calculus details, 
        please see <a target="_blank" href="https://doi.org/10.1101/2020.01.17.909838">The International Brain Laboratory et al. 2020</a>.
        The related training status indicated on the website are marked in bold and quoted.
      </p>
      <p class="detail-text">
        When the animal starts behavioral training, it first undergoes a period of habituation, then it is trained in the basic task 
        until reaching full proficiency (‘<b>trained</b>’). If the mouse does not reach proficiency in the basic task without 40 days of 
        training, it is marked as untrainable (‘<b>over 40 days</b>’). Once proficiency in the basic task is reached, the animal is trained 
        in the biased task until proficiency is achieved (‘<b>ready for ephys rig</b>’).
      </p>
      <p class="detail-text">
        Once the animal is placed on the ephys rig, it undergoes a habituation to that rig, until it is deemed ready for electrophysiological 
        recording to be conducted (‘<b>ready for recording</b>’). Once a recording has been completed, the behavior quality is validated for 
        downstream analysis (‘<b>good enough for brainwide map</b>’). The associated methods and protocols to habituate a mouse to the ephys rig and 
        perform the Neuropixels recording have not been yet publicly released; please see the figure below for a preliminary introduction 
        to our full training pipeline.
        <br>
        <img id="fig5" src="assets/images/pipeline_NP.png" alt="figure pipeline">
      </p>
    </div>
  </div>
</div>